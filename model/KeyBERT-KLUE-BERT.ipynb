{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- References\n",
    "    - [\"19-04 BERT를 이용한 키워드 추출 : 키버트(KeyBERT)\", 딥 러닝을 이용한 자연어 처리 입문 (2022)](https://wikidocs.net/159467)\n",
    "    - [ukairia777 GitHub - tensorflow-nlp-tutorial/19. Topic Modeling (LDA, BERT-Based)\n",
    "/19-5. keybert_kor.ipynb](https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/df3f84702cd4e00ec2319549a2f029c3b2d666f6/19.%20Topic%20Modeling%20(LDA%2C%20BERT-Based)/19-5.%20keybert_kor.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_MODEL_PATH = 'klue/bert-base'\n",
    "GPU_NUM = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90aIbIiLrqz3"
   },
   "source": [
    "특정 문서의 주요 정보를 이해하고자 할 때 키워드 추출을 통해서 입력 텍스트와 가장 관련이 있는 단어와 구문을 추출할 수 있습니다. 'Rake'나 'YAKE!'와 같은 키워드를 추출하는 데 사용할 수 있는 패키지가 이미 존재합니다. 그러나 이러한 모델은 일반적으로 텍스트의 통계적 특성에 기반하여 작동하며 의미적인 유사성에 대해서는 고려하지 않습니다. 의미적 유사성을 고려하기 위해서 여기서는 SBERT 임베딩을 활용하여 사용하기 쉬운 키워드 추출 알고리즘인 KeyBERT를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= f\"{GPU_NUM}\"  # Set the GPU number to use\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaxveAOApIru"
   },
   "source": [
    "# 1. 기본 KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d1eODWZ6kw2w"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(doc):\n",
    "    processed_doc = doc.strip('\"').replace(\"\\n\", \" \").strip()\n",
    "    processed_doc = re.sub('\\s+', ' ', processed_doc)\n",
    "    \n",
    "    return processed_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tokenizer(string):\n",
    "    return string.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YjY9MmrPkPbX"
   },
   "outputs": [],
   "source": [
    "doc = \"\"\"ZZ 항공 소속 B7x7 항공기가 김포공항을 이륙하여 제주공항에 접근 중 악시정으로 인하여 김해공항으로 회항하였음. 회항 후 일시적으로 목적지 제주공항 기상이 호전되어 재운항하였으나, 운항 중 다시 시정이 착륙최저치 미만상태로 악화되어 김포공항으로 회항한 후 결항 및 환불 조치하였음.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_doc = process_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UNBysM0d3g4G",
    "outputId": "02bfa026-d955-417e-d8b7-256c7cde1ab0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ZZ 항공 소속 B7x7 항공기가 김포공항을 이륙하여 제주공항에 접근 중 악시정으로 인하여 김해공항으로 회항하였음. 회항 후 일시적으로 목적지 제주공항 기상이 호전되어 재운항하였으나, 운항 중 다시 시정이 착륙최저치 미만상태로 악화되어 김포공항으로 회항한 후 결항 및 환불 조치하였음.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc = \" \".join(processed_doc.split(' '))\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Upr1ps4qswAh"
   },
   "source": [
    "여기서는 사이킷런의 CountVectorizer를 사용하여 단어를 추출합니다.  CountVectorizer를 사용하는 이유는 n_gram_range의 인자를 사용하면 단쉽게 n-gram을 추출할 수 있기 때문입니다. 예를 들어, (2, 3)으로 설정하면 결과 후보는 2개의 단어를 한 묶음으로 간주하는 bigram과 3개의 단어를 한 묶음으로 간주하는 trigram을 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C0SC908jkEfz",
    "outputId": "11b33665-9003-449a-d07e-d5efe77f48ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram 개수 : 189\n",
      "trigram 다섯개만 출력 : ['B7x7 항공기가 김포공항을' 'B7x7 항공기가 김포공항을 이륙하여' 'B7x7 항공기가 김포공항을 이륙하여 제주공항에'\n",
      " 'B7x7 항공기가 김포공항을 이륙하여 제주공항에 접근' 'B7x7 항공기가 김포공항을 이륙하여 제주공항에 접근 중']\n"
     ]
    }
   ],
   "source": [
    "n_gram_range = (3, 8)\n",
    "\n",
    "count = CountVectorizer(ngram_range=n_gram_range, lowercase=False, tokenizer=split_tokenizer, token_pattern=None).fit([tokenized_doc])\n",
    "candidates = count.get_feature_names_out()\n",
    "\n",
    "print('trigram 개수 :',len(candidates))\n",
    "print('trigram 다섯개만 출력 :',candidates[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7Dqv6jwv94E"
   },
   "source": [
    "다음으로 이제 문서와 문서로부터 추출한 키워드들을 SBERT를 통해서 수치화 할 차례입니다. 한국어를 포함하고 있는 다국어 SBERT를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'klue/bert-base'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HUGGINGFACE_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zGZKkdNdkUF_"
   },
   "outputs": [],
   "source": [
    "# 먼저 BERT 모델과 해당하는 토크나이저를 불러옵니다.\n",
    "tokenizer = BertTokenizer.from_pretrained(HUGGINGFACE_MODEL_PATH)\n",
    "model = BertModel.from_pretrained(HUGGINGFACE_MODEL_PATH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 토큰화하고, BERT가 요구하는 형식에 맞도록 텐서로 변환합니다.\n",
    "encoded_input = tokenizer(doc, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "\n",
    "# BERT 모델에 인코딩된 문장을 입력하고 결과를 얻습니다.\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# 모델 출력에서 문장 벡터를 얻습니다. 여기서는 [CLS] 토큰의 출력을 문장 벡터로 사용합니다.\n",
    "doc_embedding = model_output.last_hidden_state[:, 0, :].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UTrVhm8349Cc"
   },
   "outputs": [],
   "source": [
    "# 문장을 토큰화하고, BERT가 요구하는 형식에 맞도록 텐서로 변환합니다.\n",
    "encoded_input = tokenizer(candidates.tolist(), padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "\n",
    "# BERT 모델에 인코딩된 문장을 입력하고 결과를 얻습니다.\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# 모델 출력에서 문장 벡터를 얻습니다. 여기서는 [CLS] 토큰의 출력을 문장 벡터로 사용합니다.\n",
    "candidate_embeddings = model_output.last_hidden_state[:, 0, :].cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9KGcs62p5OG"
   },
   "source": [
    "이제 문서와 가장 유사한 키워드들을 추출합니다. 여기서는 문서와 가장 유사한 키워드들은 문서를 대표하기 위한 좋은 키워드라고 가정합니다. 상위 5개의 키워드를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8l3aJzWTkpgG"
   },
   "outputs": [],
   "source": [
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i7PkJYUAmOWN",
    "outputId": "8c21bc7d-a07a-4e31-e2b2-1c2ccfd22c96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['제주공항에 접근 중 악시정으로 인하여 김해공항으로 회항하였음. 회항',\n",
       " '악시정으로 인하여 김해공항으로 회항하였음. 회항',\n",
       " '접근 중 악시정으로 인하여 김해공항으로 회항하였음. 회항',\n",
       " '김포공항으로 회항한 후 결항 및 환불 조치하였음.',\n",
       " '이륙하여 제주공항에 접근 중 악시정으로 인하여 김해공항으로 회항하였음.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4HPcJOxmMxt"
   },
   "source": [
    "5개의 키워드가 출력되는데, 이들의 의미가 좀 비슷해보입니다. 비슷한 의미의 키워드들이 리턴되는 데는 이유가 있습니다. 당연히 이 키워드들이 문서를 가장 잘 나타내고 있기 때문입니다. 만약, 지금 출력한 것보다는 좀 더 다양한 의미의 키워드들이 출력된다면 이들을 그룹으로 본다는 관점에서는 어쩌면 해당 키워드들이 문서를 잘 나타낼 가능성이 적을 수도 있습니다. 따라서 키워드들을 다양하게 출력하고 싶다면 키워드 선정의 정확성과 키워드들의 다양성 사이의 미묘한 균형이 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HgqGxytwtFI"
   },
   "source": [
    "여기서는 다양한 키워드들을 얻기 위해서 두 가지 알고리즘을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AupvVbihwy4Z"
   },
   "source": [
    "* Max Sum Similarity  \n",
    "* Maximal Marginal Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoXd7eWFlzVx"
   },
   "source": [
    "# 2. Max Sum Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOMeOmkLl2oJ"
   },
   "source": [
    "데이터 쌍 사이의 최대 합 거리는 데이터 쌍 간의 거리가 최대화되는 데이터 쌍으로 정의됩니다. 여기서의 의도는 후보 간의 유사성을 최소화하면서 문서와의 후보 유사성을 극대화하고자 하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "7I4MpmMhk2JS"
   },
   "outputs": [],
   "source": [
    "def max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n, nr_candidates):\n",
    "    # 문서와 각 키워드들 간의 유사도\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # 코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick.\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # 각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSHRT4GyrU6T"
   },
   "source": [
    "이를 위해 상위 10개의 키워드를 선택하고 이 10개 중에서 서로 가장 유사성이 낮은 5개를 선택합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVEwuvdtoNbl"
   },
   "source": [
    "낮은 nr_candidates를 설정하면 결과는 출력된 키워드 5개는 기존의 코사인 유사도만 사용한 것과 매우 유사한 것으로 보입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8HbW4xvnw6S",
    "outputId": "015f001e-0eff-45a3-a3bd-47724ef01dd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['중 악시정으로 인하여 김해공항으로 회항하였음. 회항',\n",
       " '악화되어 김포공항으로 회항한 후 결항 및 환불 조치하였음.',\n",
       " '제주공항에 접근 중 악시정으로 인하여 김해공항으로 회항하였음. 회항',\n",
       " '김포공항으로 회항한 후 결항 및 환불 조치하였음.',\n",
       " '이륙하여 제주공항에 접근 중 악시정으로 인하여 김해공항으로 회항하였음.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyENsqxKoN39"
   },
   "source": [
    "그러나 상대적으로 높은 nr_candidates는 더 다양한 키워드 5개를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['시정이 착륙최저치 미만상태로 악화되어 김포공항으로 회항한 후 결항',\n",
       " '인하여 김해공항으로 회항하였음. 회항 후 일시적으로 목적지 제주공항',\n",
       " '회항하였음. 회항 후 일시적으로 목적지 제주공항 기상이 호전되어',\n",
       " '제주공항에 접근 중 악시정으로 인하여 김해공항으로 회항하였음.',\n",
       " '김포공항으로 회항한 후 결항 및 환불 조치하였음.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ELoogEc9oKyc",
    "outputId": "36ed2d69-9e2d-45aa-cd92-ce1a07cc5866"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['기상이 호전되어 재운항하였으나, 운항 중 다시 시정이 착륙최저치',\n",
       " '항공기가 김포공항을 이륙하여 제주공항에 접근 중 악시정으로 인하여',\n",
       " '회항한 후 결항 및 환불 조치하였음.',\n",
       " '인하여 김해공항으로 회항하였음. 회항 후 일시적으로 목적지 제주공항',\n",
       " '제주공항에 접근 중 악시정으로 인하여 김해공항으로 회항하였음.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8TJsvU-p1a3"
   },
   "source": [
    "# 3. Maximal Marginal Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6Oc7LwspRCs"
   },
   "source": [
    "결과를 다양화하는 마지막 방법은 MMR(Maximum Limit Relegance)입니다. MMR은 텍스트 요약 작업에서 중복을 최소화하고 결과의 다양성을 극대화하기 위해 노력합니다. 참고 할 수 있는 자료로 EmbedRank(https://arxiv.org/pdf/1801.04470.pdf) 라는 키워드 추출 알고리즘은 키워드를 다양화하는 데 사용할 수 있는 MMR을 구현했습니다. 먼저 문서와 가장 유사한 키워드를 선택합니다. 그런 다음 문서와 유사하고 이미 선택된 키워드와 유사하지 않은 새로운 후보를 반복적으로 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Ft7LAjDXonB_"
   },
   "outputs": [],
   "source": [
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # keywords_idx = [2]\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "\n",
    "    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\n",
    "    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # MMR을 계산\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # keywords & candidates를 업데이트\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPExUo11o213"
   },
   "source": [
    "만약 우리가 상대적으로 낮은 diversity 값을 설정한다면, 결과는 기존의 코사인 유사도만 사용한 것과 매우 유사한 것으로 보입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hfxryZSOopAy",
    "outputId": "7356d661-5598-4193-b79a-d741f71b3313"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이륙하여 제주공항에 접근 중 악시정으로 인하여 김해공항으로 회항하였음.',\n",
       " '김포공항으로 회항한 후 결항 및 환불 조치하였음.',\n",
       " '악시정으로 인하여 김해공항으로 회항하였음. 회항',\n",
       " '접근 중 악시정으로 인하여 김해공항으로 회항하였음. 회항',\n",
       " '제주공항에 접근 중 악시정으로 인하여 김해공항으로 회항하였음. 회항']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이륙하여 제주공항에 접근 중 악시정으로 인하여 김해공항으로 회항하였음.',\n",
       " '김포공항으로 회항한 후 결항 및 환불 조치하였음.',\n",
       " '회항하였음. 회항 후 일시적으로 목적지 제주공항 기상이 호전되어',\n",
       " '악시정으로 인하여 김해공항으로 회항하였음. 회항',\n",
       " '접근 중 악시정으로 인하여 김해공항으로 회항하였음. 회항']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이륙하여 제주공항에 접근 중 악시정으로 인하여 김해공항으로 회항하였음.',\n",
       " '회항하였음. 회항 후 일시적으로 목적지 제주공항 기상이 호전되어',\n",
       " '김포공항으로 회항한 후 결항 및 환불 조치하였음.',\n",
       " '시정이 착륙최저치 미만상태로 악화되어 김포공항으로 회항한 후 결항',\n",
       " '김해공항으로 회항하였음. 회항 후 일시적으로 목적지 제주공항']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이륙하여 제주공항에 접근 중 악시정으로 인하여 김해공항으로 회항하였음.',\n",
       " '제주공항 기상이 호전되어 재운항하였으나, 운항 중 다시 시정이',\n",
       " 'ZZ 항공 소속',\n",
       " '후 결항 및 환불',\n",
       " '회항하였음. 회항 후 일시적으로 목적지 제주공항 기상이 호전되어']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이륙하여 제주공항에 접근 중 악시정으로 인하여 김해공항으로 회항하였음.',\n",
       " 'ZZ 항공 소속',\n",
       " '호전되어 재운항하였으나, 운항 중 다시 시정이',\n",
       " '후 결항 및 환불',\n",
       " '제주공항 기상이 호전되어']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gL_gQ3Rlo57T"
   },
   "source": [
    "그러나 상대적으로 높은 diversity값은 다양한 키워드 5개를 만들어냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlMapb1covnK",
    "outputId": "092b28ef-20ba-4d22-f09c-066ff938cc87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이륙하여 제주공항에 접근 중 악시정으로 인하여 김해공항으로 회항하였음.',\n",
       " 'ZZ 항공 소속',\n",
       " '일시적으로 목적지 제주공항 기상이',\n",
       " '후 결항 및',\n",
       " '운항 중 다시 시정이 착륙최저치']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이륙하여 제주공항에 접근 중 악시정으로 인하여 김해공항으로 회항하였음.',\n",
       " 'ZZ 항공 소속',\n",
       " '중 다시 시정이',\n",
       " '후 결항 및',\n",
       " 'ZZ 항공 소속 B7x7 항공기가 김포공항을']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이륙하여 제주공항에 접근 중 악시정으로 인하여 김해공항으로 회항하였음.',\n",
       " '중 다시 시정이',\n",
       " 'ZZ 항공 소속',\n",
       " '후 결항 및',\n",
       " '항공 소속 B7x7 항공기가 김포공항을']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이륙하여 제주공항에 접근 중 악시정으로 인하여 김해공항으로 회항하였음.',\n",
       " '중 다시 시정이',\n",
       " 'ZZ 항공 소속',\n",
       " '후 결항 및',\n",
       " '항공 소속 B7x7 항공기가 김포공항을']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 중간 정리\n",
    "결론적으로, 테스트 케이스에 대해서는 **Max Sum Similarity** (`nr_candidates=30`)의 성능이 가장 우수한 것을 확인하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 과정을 정리하여 **document를 입력으로 받고, keyphrase를 반환하는 함수**를 만들어 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUM=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= f\"{GPU_NUM}\"  # Set the GPU number to use\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "HUGGINGFACE_MODEL_PATH = 'oneonlee/KoAirBERT'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(HUGGINGFACE_MODEL_PATH)\n",
    "model = BertModel.from_pretrained(HUGGINGFACE_MODEL_PATH).to(device)\n",
    "\n",
    "\n",
    "def process_doc(doc):\n",
    "    processed_doc = doc.strip('\"').replace(\"\\n\", \" \").strip()\n",
    "    processed_doc = re.sub('\\s+', ' ', processed_doc)\n",
    "    \n",
    "    return processed_doc\n",
    "\n",
    "def split_tokenizer(string):\n",
    "    return string.split(\" \")\n",
    "\n",
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # keywords_idx = [2]\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "\n",
    "    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\n",
    "    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # MMR을 계산\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # keywords & candidates를 업데이트\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_keyphrases(doc, model, n_gram_range=(1,10), keyword_top_n=5, mss_nr_candidates=30):\n",
    "    processed_doc = process_doc(doc)\n",
    "    tokenized_doc = \" \".join(processed_doc.split(' '))\n",
    "    \n",
    "    count = CountVectorizer(ngram_range=n_gram_range, lowercase=False, tokenizer=split_tokenizer, token_pattern=None).fit([tokenized_doc])\n",
    "\n",
    "    candidates = count.get_feature_names_out()\n",
    "\n",
    "    # 문장을 토큰화하고, BERT가 요구하는 형식에 맞도록 텐서로 변환합니다.\n",
    "    encoded_input = tokenizer(doc, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "                                                                                                          \n",
    "    # BERT 모델에 인코딩된 문장을 입력하고 결과를 얻습니다.\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    # 모델 출력에서 문장 벡터를 얻습니다. 여기서는 [CLS] 토큰의 출력을 문장 벡터로 사용합니다.\n",
    "    doc_embedding = model_output.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "    # 문장을 토큰화하고, BERT가 요구하는 형식에 맞도록 텐서로 변환합니다.\n",
    "    encoded_input = tokenizer(candidates.tolist(), padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "    # BERT 모델에 인코딩된 문장을 입력하고 결과를 얻습니다.\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    # 모델 출력에서 문장 벡터를 얻습니다. 여기서는 [CLS] 토큰의 출력을 문장 벡터로 사용합니다.\n",
    "    candidate_embeddings = model_output.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "    # keyphrases_list = mmr(doc_embedding, candidate_embeddings, candidates, top_n=keyword_top_n, diversity=mmr_diversity)    \n",
    "    keyphrases_list = max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=keyword_top_n, nr_candidates=mss_nr_candidates)    \n",
    "    \n",
    "    return keyphrases_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "아내에게 집에서 저녁식사를 하겠다고 하고, 집을 나선 헬리콥터 조종사는 '모두를 행복하게' 해줄 생각으로 집으로 돌아가기로 마음먹었지만 그 결정은 그다지 현명하지 못한 결정이 되어버렸다. 나는 헬리콥터를 집 뒤에 있는 마을 공터에 착륙시켰다. 공터 면적은 충분히 넓었고, 착륙은 교과서대로 진행하였으며, 착륙장소 주변엔 아무도 없었다. 같은 장소는 아니지만 전에도 아무 문제없이 착륙해 본 경험이 있어서 걱정 없이 착륙했는데. 착륙하고 보니 큰 일이 벌어져 있었다. \n",
    "지나가던 구급차가 내 헬리콥터가 착륙하는 장면을 보고는 경찰서에 추락신고를 한 것이다. 수 분만에 경찰차, 소방차, 기자들이 모여들었다. \n",
    "단지 가족과 저녁식사를 하려했다는 이유는 관계당국에 설득력 있는 이유가 되지 못했고 여러 가지 주정부 법에 따라 벌금형이 내려졌다. \n",
    "착륙 전 충분한 '주의'를 기울였음에도 불구하고 FAR91.13 조차도 위반했다고 한다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"법에 따라 벌금형이 내려졌다. 착륙 전 충분한 '주의'를 기울였음에도\",\n",
       " \"'주의'를 기울였음에도 불구하고 FAR91.13 조차도 위반했다고 한다.\",\n",
       " \"따라 벌금형이 내려졌다. 착륙 전 충분한 '주의'를 기울였음에도 불구하고 FAR91.13\",\n",
       " '착륙하는 장면을 보고는 경찰서에 추락신고를 한 것이다. 수 분만에 경찰차,',\n",
       " '본 경험이 있어서 걱정 없이 착륙했는데. 착륙하고']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_to_keyphrases(doc, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 테스트셋 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/rawdata/GYRO_testset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 219/219 [13:21<00:00,  3.66s/it]\n"
     ]
    }
   ],
   "source": [
    "predicted_keyphrases_list = []\n",
    "for i in tqdm(range(len(df))):\n",
    "    doc = df[\"본문\"][i]\n",
    "    predicted_keyphrase = doc_to_keyphrases(doc, model=model)\n",
    "    predicted_keyphrases_list.append('\"' + '\", \"'.join(predicted_keyphrase) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"KeyBERT 예측 키워드\"] = predicted_keyphrases_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/prediction/KeyBERT-BERT.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "KeyBert_kor",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
